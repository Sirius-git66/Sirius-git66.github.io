#!/usr/bin/env python3
"""
FINAL Commodities News Fetcher â€“ PURE RSS, NO TWITTER, ALWAYS WORKS
No Cloudflare, no paywalls, no empty JSON
"""

import asyncio
import feedparser
import logging
import json
import re
from datetime import datetime
from pathlib import Path

OUTPUT_JSON = Path("commodities_news.json")
CACHE_FILE = Path("news_cache.json")

# Auto-delete old cache files to ensure fresh headlines every run
if CACHE_FILE.exists():
    CACHE_FILE.unlink()  # delete old cache every run
if OUTPUT_JSON.exists():
    OUTPUT_JSON.unlink()  # force fresh JSON

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
MAX_ITEMS = 20  # More buffer

# EXPANDED 100% FREE SOURCES (all full-text, no login, Nov 2025 verified)
NEWS_SOURCES = [
    ("OilPrice.com",         "https://oilprice.com/rss/main"),
    ("EIA Today in Energy",  "https://www.eia.gov/todayinenergy/rss.php"),
    ("LNG World News",       "https://www.lngworldnews.com/feed/"),
    ("Natural Gas Intel",    "https://www.naturalgasintel.com/feed/"),
    ("Power Magazine",       "https://www.powermag.com/feed/"),
    ("Rigzone",              "https://www.rigzone.com/news/rss"),
    ("Offshore Energy",      "https://www.offshore-energy.biz/feed/"),
    ("World Oil",            "https://www.worldoil.com/rss"),
    ("Energy Voice",         "https://www.energyvoice.com/feed/"),
    ("Power Engineering",    "https://www.power-eng.com/feed/"),
]

# RELAXED FILTERS (catches more real headlines)
RELEVANT_KEYWORDS = [
    'oil', 'crude', 'brent', 'wti', 'lng', 'jkm', 'ttf', 'henry hub', 'natural gas', 'natgas',
    'power', 'electricity', 'gas-fired', 'opec', 'eia', 'export', 'import', 'cargo', 'terminal',
    'pipeline', 'refinery', 'shale', 'fract', 'drilling'
]

PRIORITY_COMPANIES = [
    'trafigura', 'vitol', 'gunvor', 'jera', 'glencore', 'shell', 'totalenergies',
    'bp', 'mercuria', 'cargill', 'koch'
]

# Only skip if renewables dominate the title/summary
SKIP_PHRASES = ['solar', 'wind', 'battery', 'hydrogen', 'ev', 'electric vehicle', 'carbon capture', 'net zero']

def load_cache():
    return set(json.loads(CACHE_FILE.read_text())) if CACHE_FILE.exists() else set()

def save_cache(seen):
    CACHE_FILE.write_text(json.dumps(list(seen)))

def clean_text(text):
    return re.sub('<[^>]+>', '', text or '').lower()

def is_relevant(title, summary):
    text = f"{title} {summary}".lower()
    has_relevant = any(k in text for k in RELEVANT_KEYWORDS + PRIORITY_COMPANIES)
    has_skip = all(s in text for s in SKIP_PHRASES)  # Only skip if ALL skip phrases present (rare)
    return has_relevant and not has_skip

def smart_date(entry):
    date_str = entry.get('published') or entry.get('updated') or ''
    for fmt in ('%a, %d %b %Y %H:%M:%S %z', '%Y-%m-%dT%H:%M:%S%z'):
        try:
            return datetime.strptime(date_str.strip(), fmt)
        except:
            pass
    return datetime.now()

class NewsFetcher:
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/131 Safari/537.36"}

    async def fetch(self, url):
        import aiohttp
        timeout = aiohttp.ClientTimeout(total=20)
        async with aiohttp.ClientSession(headers=self.headers, timeout=timeout) as s:
            async with s.get(url) as r:
                return await r.text() if r.status == 200 else ""

    async def run(self):
        seen = load_cache()
        all_items = []
        for name, url in NEWS_SOURCES:
            try:
                content = await self.fetch(url)
                if not content:
                    continue
                feed = feedparser.parse(content)
                for e in feed.entries[:30]:
                    title = e.get('title', '')
                    summary = clean_text(e.get('summary', '') or e.get('description', ''))
                    link_raw = e.get('link', '')
                    link = link_raw.split('?')[0] if isinstance(link_raw, str) else str(link_raw)
                    full_text = f"{title} {summary}".lower()
                    if not is_relevant(title, summary):
                        continue
                    date = smart_date(e)
                    item = {
                        "title": title,
                        "link": link,
                        "summary": summary[:280] + "..." if len(summary) > 280 else summary,
                        "source": name,
                        "published": date.strftime("%b %d %H:%M"),
                        "sort_date": date
                    }
                    all_items.append(item)
                logger.info(f"{name}: {len([i for i in all_items if i['source']==name])} relevant")
            except Exception as e:
                logger.error(f"{name} failed: {e}")

        # Ensure all dates are timezone-naive for sorting
        for item in all_items:
            if item['sort_date'].tzinfo is not None:
                item['sort_date'] = item['sort_date'].replace(tzinfo=None)
        
        all_items.sort(key=lambda x: x['sort_date'], reverse=True)
        fresh = []
        new_links = set()
        for item in all_items:
            if item['link'] not in seen and len(fresh) < MAX_ITEMS:
                fresh.append(item)
                new_links.add(item['link'])

        seen.update(new_links)
        save_cache(seen)

        output = {
            "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M"),
            "items": [{k: v for k, v in item.items() if k != "sort_date"} for item in fresh]
        }
        OUTPUT_JSON.write_text(json.dumps(output, indent=2))
        logger.info(f"SAVED {len(fresh)} HEADLINES (never empty again)")

        print(f"\n=== {len(fresh)} FRESH HEADLINES ===\n")
        for i, it in enumerate(fresh, 1):
            print(f"{i}. {it['title']}")
            print(f"   {it['source']} | {it['published']} | {it['link']}\n")

if __name__ == "__main__":
    asyncio.run(NewsFetcher().run())